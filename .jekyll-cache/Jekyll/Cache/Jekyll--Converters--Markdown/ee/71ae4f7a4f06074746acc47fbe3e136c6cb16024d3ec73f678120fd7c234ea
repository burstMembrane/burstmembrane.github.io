I"±	<p><em>State Machine</em> is a multidisciplinary exploration into artificial simulation, memory and perception. It contrasts the passivity of the machine with the sentimentality of the human experience.</p>

<p><img src="images/statemachine/sm3.png" alt="statemachine 1" /></p>
<h3 id="state-machine-terminal">State Machine: Terminal</h3>

<p>At the beginning of 2018 I downloaded all of my Facebook data. A decades worth of messages, check ins, statuses, photos, videos and assorted ephemera were all bundled and emailed to me as a huge 5gb document.</p>

<p>Raking through this digital
archive of my online self, I began wondering what could be pieced together by the analysis and re-purposing of this data. Could I construct a believable simulation of myself, or at the very least a machine which could take this input, process it in a basic simulation of a cognitive system, and output it to create something new?</p>

<p>Following this prompt I have refined a number of outcomes which reflect on the contrasts and similarities between the simulated and â€œrealâ€ brain.</p>

<p>Taking the form of online content, interactive installation and sound composition. State machine is an ongoing project in which I attempt to see what can be recreated from all of this data.</p>

<p><img src="images/statemachine/sm1.png" alt="statemachine 1" /></p>
<h3 id="screenshot-state-machine-website">Screenshot: State Machine Website</h3>

<p>The website - found <a href="http://liamfpower.com/codesnippets/terminalboredom/">here</a> takes 10 years of Facebook messenger logs, and runs them through a series of markov chains.
These chains take the weighted probablities attached to words and add them together, creating a set of probabilities for which words will follow which.</p>

<p>For instance, â€œIâ€ is more likely to follow â€œIfâ€ than â€œBananaâ€, so â€œIâ€ has a higher probability than â€œBananaâ€ of occurring after the word â€œIfâ€. By chaining together a large number of these probabilities, somewhat meaningful sentences can be constructed.</p>

<p>This project continues with the creation of a neural network which uses a LSTM text generation algorithm in order to generate new text from a dataset. In this case, the data was trained on my recently completed Honours Thesis in an attempt to learn to synthesize institutionalized art jargon at record speeds. My research continues on this subject as newly acquired coding skills open new avenues for exploration.</p>

:ET